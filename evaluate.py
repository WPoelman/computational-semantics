#!/usr/bin/env python

"""
Filename:   evaluate.py
Date:       12-1-2022
Authors:    Wessel Poelman, Esther Ploeger, Frank van den Berg
Description:
    This script script outputs evaluation metrics for Word Sense Disambiguation (WSD) prediction.
    It expects as input the .conll files of the gold standard and prints accuracy scores as output.
    .
"""


import argparse
import pickle
from src.conll import AnnCategory, ConllDataset
from src.utils import accuracy_score


def create_arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("-e", "--evaluation_file", default='data/dev.conll', type=str,
                        help="Path to evaluation file")
    parser.add_argument("-p", "--prediction_file", default='results/baseline_predictions_dev.pickle', type=str,
                        help="List of predictions as generated by a system")

    args = parser.parse_args()
    return args


def evaluate_synsets(gold, predictions, show_outputs=False):
    """Print accuracy on synset level, given predictions and the gold standard"""

    if show_outputs:
        # Display the synsets that are compared, with either 0 (correct) or 1 (wrong)
        for gold_sns, pred_sns in zip(gold, predictions):
            print(f"{gold_sns:20} {pred_sns:20} {1 if gold_sns == pred_sns else 0}")

    print(f'Correct synsets (accuracy): {accuracy_score(gold, predictions)}\n')


def evaluate_sentences(gold, predictions):
    """Print accuracy on sentence level, given predictions and the gold standard"""
    accuracies = []
    g = []
    cleaned_pred = []

    for gold_sent, pred_sent in zip(gold, predictions):
        gold_sent = remove_sns_none(gold_sent, mode='single')
        g.append(gold_sent)
        pred_sent = remove_sns_none(pred_sent, mode="single")
        cleaned_pred.append(pred_sent)
        all_gold, all_predictions = [], []
        for sense, pred in zip(gold_sent, pred_sent):
            if sense:
                all_gold.append(sense)
                all_predictions.append(pred)

        if all_gold and all_predictions:
            accuracies.append(accuracy_score(all_gold, all_predictions))

    print(f"Mean number of correct synsets (accuracy) per sentence: {sum(accuracies) / len(accuracies)}")
    print(f"Fully correct sentences (accuracy): {accuracy_score(g, cleaned_pred)}\n")


def remove_sns_none(full_sns, mode="multiple"):
    """ Remove SNS_NONE from lists, as it distorts scores"""
    filtered = []
    if mode == "multiple":
        for sns_sent in full_sns:
            for sense in sns_sent:
                if sense:
                    filtered.append(sense)
    elif mode == "single":
        for sense in full_sns:
            if sense:
                filtered.append(sense)

    return filtered


def main():
    args = create_arg_parser()

    # Get gold labels / evaluation dataset
    gold_data = ConllDataset(args.evaluation_file)
    gold_labels_full = gold_data.get_category(AnnCategory.SNS)

    # Get predictions
    pred_file = args.prediction_file
    with open(pred_file, 'rb') as pred:
        predictions_full = pickle.load(pred)

    # Filter SNS_NONE from full synset collections
    gold_labels = remove_sns_none(gold_labels_full)
    predictions = remove_sns_none(predictions_full)

    # Print evaluation
    print("\n********** EVALUATION **********\n")

    print("*** MICRO: EVALUATION ON SYNSET LEVEL ***")
    evaluate_synsets(gold_labels, predictions, show_outputs=False)

    print("*** MICRO: EVALUATION ON SENTENCE LEVEL ***")
    evaluate_sentences(gold_labels_full, predictions_full)


if __name__ == "__main__":
    main()
