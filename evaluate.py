#!/usr/bin/env python

"""
Filename:   evaluate.py
Date:       12-1-2022
Authors:    Wessel Poelman, Esther Ploeger, Frank van den Berg
Description:
    This script script outputs evaluation metrics for Word Sense Disambiguation
    (WSD) prediction. It expects as input the .conll files of the gold standard 
    and prints accuracy scores as output.
"""


import argparse
import pickle

from src.conll import AnnCategory, ConllDataset
from src.utils import accuracy_score


def create_arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("-e", "--evaluation_file",
                        default='data/dev.conll', type=str,
                        help="Path to evaluation file")
    parser.add_argument("-p", "--prediction_file", type=str,
                        default='results/baseline_predictions_dev.pickle',
                        help="List of predictions as generated by a system")
    return parser.parse_args()


def evaluate_synsets(gold, predictions, show_outputs=False):
    """Print accuracy on synset level, given predictions and the gold standard"""

    if show_outputs:
        # Display the synsets that are compared, with either 0 (correct) or 1 (wrong)
        for gold_sns, pred_sns in zip(gold, predictions):
            print(f"{gold_sns} {pred_sns} {1 if gold_sns == pred_sns else 0}")

    print(f'Correct synsets (accuracy): {accuracy_score(gold, predictions)}\n')


def evaluate_sentences(gold, predictions):
    """Print accuracy on sentence level, given predictions and the gold standard"""
    accuracies = []
    cleaned_gold = []
    cleaned_pred = []

    for gold_sent, pred_sent in zip(gold, predictions):
        gold_sent = remove_sns_none(gold_sent, mode='single')
        cleaned_gold.append(gold_sent)

        pred_sent = remove_sns_none(pred_sent, mode="single")
        cleaned_pred.append(pred_sent)

        if gold_sent and pred_sent:
            accuracies.append(accuracy_score(gold_sent, pred_sent))

    m_acc = sum(accuracies) / len(accuracies)
    sents_acc = accuracy_score(cleaned_gold, cleaned_pred)

    print(f"Mean number of correct synsets (accuracy) per sentence: {m_acc}")
    print(f"Fully correct sentences (accuracy): {sents_acc}\n")


def remove_sns_none(full_sns, mode="multiple"):
    """ Remove SNS_NONE from lists, as it distorts scores"""
    filtered = []
    if mode == "multiple":
        for sns_sent in full_sns:
            for sense in sns_sent:
                if sense:
                    filtered.append(sense)
    elif mode == "single":
        for sense in full_sns:
            if sense:
                filtered.append(sense)

    return filtered


def main():
    args = create_arg_parser()

    # Get gold labels / evaluation dataset
    gold_data = ConllDataset(args.evaluation_file)
    gold_labels_full = gold_data.get_category(AnnCategory.SNS)

    # Get predictions
    pred_file = args.prediction_file
    with open(pred_file, 'rb') as pred:
        predictions_full = pickle.load(pred)

    # Filter SNS_NONE from full synset collections
    gold_labels = remove_sns_none(gold_labels_full)
    predictions = remove_sns_none(predictions_full)

    # Print evaluation
    print("\n********** EVALUATION **********\n")

    print("*** MICRO: EVALUATION ON SYNSET LEVEL ***")
    evaluate_synsets(gold_labels, predictions, show_outputs=False)

    print("*** MICRO: EVALUATION ON SENTENCE LEVEL ***")
    evaluate_sentences(gold_labels_full, predictions_full)


if __name__ == "__main__":
    main()
