#!/usr/bin/env python

"""
Filename:   evaluate.py
Date:       12-1-2022
Authors:    Wessel Poelman, Esther Ploeger, Frank van den Berg
Description:
    This script script outputs evaluation metrics for Word Sense Disambiguation (WSD) prediction.
    It expects as input the .conll files of the gold standard and
    .
"""


import argparse
import pickle
from src.conll import SNS_NONE, ConllDataset
from src.utils import accuracy_score


def create_arg_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("-e", "--evaluation_file", default='dev', type=str,
                        help="Type of evaluation file: dev, eval or test")
    parser.add_argument("-p", "--prediction_file", default='results/baseline_predictions.pickle', type=str,
                        help="List of predictions as generated by a system")

    args = parser.parse_args()
    return args


def evaluate_synsets(gold, predictions, show_outputs=False):
    """Print accuracy on synset level, given predictions and the gold standard"""

    # Filter SNS_NONE senses from both lists, as it distorts scores
    all_gold, all_predictions = [], []
    for gold_sent, pred_sent in zip(gold, predictions):
        for sense, pred in zip(gold_sent, pred_sent):
            if sense != SNS_NONE:
                all_gold.append(sense)
                all_predictions.append(pred)

    if show_outputs:
        # Display the synsets that are compared, with either 0 (correct) or 1 (wrong)
        for gold_sns, pred_sns in zip(all_gold, all_predictions):
            print(f"{gold_sns:20} {pred_sns:20} {1 if gold_sns == pred_sns else 0}")

    print(f'Correct synsets (accuracy): {accuracy_score(all_gold, all_predictions)}\n')


def evaluate_sentences(gold, predictions):
    """Print accuracy on sentence level, given predictions and the gold standard"""
    accuracies = []

    # Filter SNS_NONE senses from both lists, as it distorts scores
    for gold_sent, pred_sent in zip(gold, predictions):
        all_gold, all_predictions = [], []
        for sense, pred in zip(gold_sent, pred_sent):
            if sense != SNS_NONE:
                all_gold.append(sense)
                all_predictions.append(pred)

        if all_gold and all_predictions:
            accuracies.append(accuracy_score(all_gold, all_predictions))

    print(f"Mean number of correct synsets (accuracy) per sentence: {sum(accuracies) / len(accuracies)}")
    print(f"Fully correct sentences (accuracy): {accuracy_score(gold, predictions)}\n")  # removing "O" not necessary


def main():
    args = create_arg_parser()

    # Get gold labels / evaluation dataset
    if args.evaluation_file in ["dev", "eval", "test"]:
        gold_data = ConllDataset("data/" + args.evaluation_file + ".conll")
    else:
        raise ValueError("The evaluation set must be specified as one of the following: 'dev', 'eval', 'test'")
    gold_labels = [doc.sns for doc in gold_data.docs]

    # Get predictions
    pred_file = args.prediction_file
    with open(pred_file, 'rb') as pred:
        predictions = pickle.load(pred)

    # Print evaluation
    print("\n********** EVALUATION **********\n")

    print("*** MICRO: EVALUATION ON SYNSET LEVEL ***")
    evaluate_synsets(gold_labels, predictions, show_outputs=False)

    print("*** MICRO: EVALUATION ON SENTENCE LEVEL ***")
    evaluate_sentences(gold_labels, predictions)


if __name__ == "__main__":
    main()
